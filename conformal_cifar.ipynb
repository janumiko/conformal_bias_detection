{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from construct_model import construct_model, register_models\n",
    "from constants import C10_CLASSES_DICT, C100_CLASSES_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"datasets\"\n",
    "DATASET = \"CIFAR100\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "c10_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]\n",
    "        ),  # normalize the cifar10 images\n",
    "    ]\n",
    ")\n",
    "\n",
    "c10_test_dataset = datasets.CIFAR10(\n",
    "    root=DATASETS_PATH,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=c10_test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# cifar 100\n",
    "c100_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761]\n",
    "        ),  # normalize the cifar100 images\n",
    "    ]\n",
    ")\n",
    "\n",
    "c100_test_dataset = datasets.CIFAR100(\n",
    "    root=DATASETS_PATH,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=c100_test_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATASET == \"CIFAR100\":\n",
    "    test_dataset = c100_test_dataset\n",
    "    classes_dict = C100_CLASSES_DICT\n",
    "    checkpoint_path = \"resnet18_cifar100.pth\"\n",
    "    model_name = \"resnet18_cifar\"\n",
    "    num_of_classes = 100\n",
    "elif DATASET == \"CIFAR10\":\n",
    "    test_dataset = c10_test_dataset\n",
    "    classes_dict = C10_CLASSES_DICT\n",
    "    checkpoint_path = \"resnet18_cifar10.pth\"\n",
    "    model_name = \"resnet18_cifar\"\n",
    "    num_of_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of validation dataset: 5000\n"
     ]
    }
   ],
   "source": [
    "# Get the indices of the test_dataset\n",
    "indices = list(range(len(test_dataset)))\n",
    "\n",
    "# Split the indices into test_indices and validation_indices\n",
    "test_indices, validation_indices = train_test_split(\n",
    "    indices, test_size=0.5, random_state=42, stratify=test_dataset.targets\n",
    ")\n",
    "\n",
    "# Create validation and test datasets\n",
    "validation_dataset = Subset(test_dataset, validation_indices)\n",
    "test_dataset = Subset(test_dataset, test_indices)\n",
    "\n",
    "print(f\"Size of validation dataset: {len(validation_dataset)}\")\n",
    "\n",
    "calibration_dl = DataLoader(\n",
    "    validation_dataset, batch_size=256, shuffle=False, pin_memory=True\n",
    ")\n",
    "test_dl = DataLoader(test_dataset, batch_size=256, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = construct_model(\n",
    "    model_name, num_classes=num_of_classes, checkpoint_path=checkpoint_path\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of apple: 90.00%\n",
      "Accuracy of aquarium_fish: 86.00%\n",
      "Accuracy of baby: 72.00%\n",
      "Accuracy of bear: 52.00%\n",
      "Accuracy of beaver: 66.00%\n",
      "Accuracy of bed: 84.00%\n",
      "Accuracy of bee: 66.00%\n",
      "Accuracy of beetle: 72.00%\n",
      "Accuracy of bicycle: 94.00%\n",
      "Accuracy of bottle: 90.00%\n",
      "Accuracy of bowl: 60.00%\n",
      "Accuracy of boy: 38.00%\n",
      "Accuracy of bridge: 86.00%\n",
      "Accuracy of bus: 60.00%\n",
      "Accuracy of butterfly: 72.00%\n",
      "Accuracy of camel: 84.00%\n",
      "Accuracy of can: 74.00%\n",
      "Accuracy of castle: 82.00%\n",
      "Accuracy of caterpillar: 68.00%\n",
      "Accuracy of cattle: 62.00%\n",
      "Accuracy of chair: 96.00%\n",
      "Accuracy of chimpanzee: 86.00%\n",
      "Accuracy of clock: 78.00%\n",
      "Accuracy of cloud: 94.00%\n",
      "Accuracy of cockroach: 82.00%\n",
      "Accuracy of couch: 66.00%\n",
      "Accuracy of crab: 52.00%\n",
      "Accuracy of crocodile: 62.00%\n",
      "Accuracy of cup: 88.00%\n",
      "Accuracy of dinosaur: 78.00%\n",
      "Accuracy of dolphin: 68.00%\n",
      "Accuracy of elephant: 78.00%\n",
      "Accuracy of flatfish: 64.00%\n",
      "Accuracy of forest: 64.00%\n",
      "Accuracy of fox: 82.00%\n",
      "Accuracy of girl: 62.00%\n",
      "Accuracy of hamster: 90.00%\n",
      "Accuracy of house: 74.00%\n",
      "Accuracy of kangaroo: 82.00%\n",
      "Accuracy of keyboard: 92.00%\n",
      "Accuracy of lamp: 68.00%\n",
      "Accuracy of lawn_mower: 90.00%\n",
      "Accuracy of leopard: 58.00%\n",
      "Accuracy of lion: 88.00%\n",
      "Accuracy of lizard: 60.00%\n",
      "Accuracy of lobster: 68.00%\n",
      "Accuracy of man: 62.00%\n",
      "Accuracy of maple_tree: 64.00%\n",
      "Accuracy of motorcycle: 86.00%\n",
      "Accuracy of mountain: 86.00%\n",
      "Accuracy of mouse: 64.00%\n",
      "Accuracy of mushroom: 68.00%\n",
      "Accuracy of oak_tree: 58.00%\n",
      "Accuracy of orange: 96.00%\n",
      "Accuracy of orchid: 84.00%\n",
      "Accuracy of otter: 56.00%\n",
      "Accuracy of palm_tree: 92.00%\n",
      "Accuracy of pear: 86.00%\n",
      "Accuracy of pickup_truck: 84.00%\n",
      "Accuracy of pine_tree: 60.00%\n",
      "Accuracy of plain: 90.00%\n",
      "Accuracy of plate: 76.00%\n",
      "Accuracy of poppy: 78.00%\n",
      "Accuracy of porcupine: 74.00%\n",
      "Accuracy of possum: 50.00%\n",
      "Accuracy of rabbit: 60.00%\n",
      "Accuracy of raccoon: 76.00%\n",
      "Accuracy of ray: 66.00%\n",
      "Accuracy of road: 94.00%\n",
      "Accuracy of rocket: 78.00%\n",
      "Accuracy of rose: 78.00%\n",
      "Accuracy of sea: 86.00%\n",
      "Accuracy of seal: 54.00%\n",
      "Accuracy of shark: 58.00%\n",
      "Accuracy of shrew: 56.00%\n",
      "Accuracy of skunk: 86.00%\n",
      "Accuracy of skyscraper: 98.00%\n",
      "Accuracy of snail: 74.00%\n",
      "Accuracy of snake: 64.00%\n",
      "Accuracy of spider: 82.00%\n",
      "Accuracy of squirrel: 68.00%\n",
      "Accuracy of streetcar: 80.00%\n",
      "Accuracy of sunflower: 90.00%\n",
      "Accuracy of sweet_pepper: 68.00%\n",
      "Accuracy of table: 74.00%\n",
      "Accuracy of tank: 88.00%\n",
      "Accuracy of telephone: 70.00%\n",
      "Accuracy of television: 86.00%\n",
      "Accuracy of tiger: 68.00%\n",
      "Accuracy of tractor: 88.00%\n",
      "Accuracy of train: 78.00%\n",
      "Accuracy of trout: 86.00%\n",
      "Accuracy of tulip: 66.00%\n",
      "Accuracy of turtle: 68.00%\n",
      "Accuracy of wardrobe: 96.00%\n",
      "Accuracy of whale: 80.00%\n",
      "Accuracy of willow_tree: 78.00%\n",
      "Accuracy of wolf: 74.00%\n",
      "Accuracy of woman: 62.00%\n",
      "Accuracy of worm: 82.00%\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "class_correct = [0 for _ in range(num_of_classes)]\n",
    "class_total = [0 for _ in range(num_of_classes)]\n",
    "\n",
    "resnet18.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_dl:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = resnet18(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "# Print accuracy for each class\n",
    "for i in range(num_of_classes):\n",
    "    print(\n",
    "        f\"Accuracy of {classes_dict[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_coverage(\n",
    "    model: torch.nn.Module,\n",
    "    test_dl: DataLoader,\n",
    "    class_thresholds: dict[int, float],\n",
    "    num_of_classes: int,\n",
    "    classes_dict: dict[int, str],\n",
    "):\n",
    "    class_correct = [0 for _ in range(num_of_classes)]\n",
    "    class_total = [0 for _ in range(num_of_classes)]\n",
    "    set_sizes = []\n",
    "\n",
    "    model.eval()\n",
    "    for images, labels in test_dl:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predicted_prob = model(images).softmax(dim=1)\n",
    "        scores = 1 - predicted_prob\n",
    "\n",
    "        sets = scores <= torch.tensor(\n",
    "            [class_thresholds[i] for i in range(num_of_classes)], device=device\n",
    "        )\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            class_total[label] += 1\n",
    "            if sets[i, label]:\n",
    "                class_correct[label] += 1\n",
    "        set_sizes.extend(sets.sum(dim=1).cpu().numpy())\n",
    "\n",
    "    # Print coverage for each class\n",
    "    for i in range(num_of_classes):\n",
    "        if class_total[i] > 0:\n",
    "            print(\n",
    "                f\"Coverage of {classes_dict[i]}: {100 * class_correct[i] / class_total[i]:.2f}%\"\n",
    "            )\n",
    "    print(f\"Average coverage: {100 * np.sum(class_correct) / np.sum(class_total):.2f}%\")\n",
    "    print(f\"Average set size: {np.mean(set_sizes):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calibrate the model\n",
    "conformity_scores = []\n",
    "\n",
    "for images, labels in calibration_dl:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    predicted_prob = resnet18(images).softmax(dim=1)\n",
    "    scores = 1 - predicted_prob[range(len(labels)), labels]\n",
    "\n",
    "    conformity_scores.extend(scores.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0017024875, 0.00048708916, 0.001824975, 0.2402181, 0.025772631]\n"
     ]
    }
   ],
   "source": [
    "print(conformity_scores[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile: 0.95019\n",
      "Threshold: 0.9948265487289428\n"
     ]
    }
   ],
   "source": [
    "COVERAGE = 0.95\n",
    "\n",
    "quantile = COVERAGE * ((len(calibration_dl.dataset) + 1) / len(calibration_dl.dataset))\n",
    "print(f\"Quantile: {quantile}\")\n",
    "threshold = np.quantile(conformity_scores, quantile)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "\n",
    "single_threshold = {i: threshold for i in range(num_of_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of apple: 100.00%\n",
      "Coverage of aquarium_fish: 98.00%\n",
      "Coverage of baby: 90.00%\n",
      "Coverage of bear: 88.00%\n",
      "Coverage of beaver: 94.00%\n",
      "Coverage of bed: 94.00%\n",
      "Coverage of bee: 94.00%\n",
      "Coverage of beetle: 98.00%\n",
      "Coverage of bicycle: 100.00%\n",
      "Coverage of bottle: 96.00%\n",
      "Coverage of bowl: 90.00%\n",
      "Coverage of boy: 88.00%\n",
      "Coverage of bridge: 96.00%\n",
      "Coverage of bus: 90.00%\n",
      "Coverage of butterfly: 92.00%\n",
      "Coverage of camel: 94.00%\n",
      "Coverage of can: 98.00%\n",
      "Coverage of castle: 94.00%\n",
      "Coverage of caterpillar: 90.00%\n",
      "Coverage of cattle: 86.00%\n",
      "Coverage of chair: 98.00%\n",
      "Coverage of chimpanzee: 96.00%\n",
      "Coverage of clock: 94.00%\n",
      "Coverage of cloud: 98.00%\n",
      "Coverage of cockroach: 98.00%\n",
      "Coverage of couch: 94.00%\n",
      "Coverage of crab: 98.00%\n",
      "Coverage of crocodile: 94.00%\n",
      "Coverage of cup: 96.00%\n",
      "Coverage of dinosaur: 94.00%\n",
      "Coverage of dolphin: 90.00%\n",
      "Coverage of elephant: 98.00%\n",
      "Coverage of flatfish: 90.00%\n",
      "Coverage of forest: 90.00%\n",
      "Coverage of fox: 96.00%\n",
      "Coverage of girl: 86.00%\n",
      "Coverage of hamster: 96.00%\n",
      "Coverage of house: 98.00%\n",
      "Coverage of kangaroo: 98.00%\n",
      "Coverage of keyboard: 98.00%\n",
      "Coverage of lamp: 92.00%\n",
      "Coverage of lawn_mower: 98.00%\n",
      "Coverage of leopard: 96.00%\n",
      "Coverage of lion: 100.00%\n",
      "Coverage of lizard: 98.00%\n",
      "Coverage of lobster: 90.00%\n",
      "Coverage of man: 92.00%\n",
      "Coverage of maple_tree: 94.00%\n",
      "Coverage of motorcycle: 100.00%\n",
      "Coverage of mountain: 98.00%\n",
      "Coverage of mouse: 96.00%\n",
      "Coverage of mushroom: 96.00%\n",
      "Coverage of oak_tree: 92.00%\n",
      "Coverage of orange: 100.00%\n",
      "Coverage of orchid: 96.00%\n",
      "Coverage of otter: 86.00%\n",
      "Coverage of palm_tree: 100.00%\n",
      "Coverage of pear: 100.00%\n",
      "Coverage of pickup_truck: 96.00%\n",
      "Coverage of pine_tree: 94.00%\n",
      "Coverage of plain: 98.00%\n",
      "Coverage of plate: 94.00%\n",
      "Coverage of poppy: 98.00%\n",
      "Coverage of porcupine: 96.00%\n",
      "Coverage of possum: 82.00%\n",
      "Coverage of rabbit: 88.00%\n",
      "Coverage of raccoon: 92.00%\n",
      "Coverage of ray: 98.00%\n",
      "Coverage of road: 100.00%\n",
      "Coverage of rocket: 96.00%\n",
      "Coverage of rose: 94.00%\n",
      "Coverage of sea: 98.00%\n",
      "Coverage of seal: 92.00%\n",
      "Coverage of shark: 96.00%\n",
      "Coverage of shrew: 90.00%\n",
      "Coverage of skunk: 96.00%\n",
      "Coverage of skyscraper: 100.00%\n",
      "Coverage of snail: 98.00%\n",
      "Coverage of snake: 94.00%\n",
      "Coverage of spider: 98.00%\n",
      "Coverage of squirrel: 88.00%\n",
      "Coverage of streetcar: 98.00%\n",
      "Coverage of sunflower: 98.00%\n",
      "Coverage of sweet_pepper: 90.00%\n",
      "Coverage of table: 98.00%\n",
      "Coverage of tank: 98.00%\n",
      "Coverage of telephone: 96.00%\n",
      "Coverage of television: 98.00%\n",
      "Coverage of tiger: 96.00%\n",
      "Coverage of tractor: 100.00%\n",
      "Coverage of train: 100.00%\n",
      "Coverage of trout: 96.00%\n",
      "Coverage of tulip: 90.00%\n",
      "Coverage of turtle: 88.00%\n",
      "Coverage of wardrobe: 100.00%\n",
      "Coverage of whale: 90.00%\n",
      "Coverage of willow_tree: 96.00%\n",
      "Coverage of wolf: 94.00%\n",
      "Coverage of woman: 96.00%\n",
      "Coverage of worm: 98.00%\n",
      "Average coverage: 94.90%\n",
      "Average set size: 5.10\n"
     ]
    }
   ],
   "source": [
    "calculate_class_coverage(\n",
    "    resnet18, test_dl, single_threshold, num_of_classes, classes_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class balanced conformal prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize conformity scores list for each class\n",
    "class_conformity_scores = {i: [] for i in range(num_of_classes)}\n",
    "\n",
    "for images, labels in calibration_dl:\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    predicted_prob = resnet18(images).softmax(dim=1)\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        class_conformity_scores[labels[i].item()].append(\n",
    "            1 - predicted_prob[i, labels[i]].item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9659874893398956, 1: 0.8374806193709374, 2: 0.993297046628315, 3: 0.9968778236630605, 4: 0.9989432904410059, 5: 0.9593531790254638, 6: 0.9718274052161724, 7: 0.9993298088825541, 8: 0.9527631902229041, 9: 0.9931393048067112, 10: 0.99926412077251, 11: 0.998794112010859, 12: 0.986573983468581, 13: 0.9972474765734515, 14: 0.997645575151546, 15: 0.9625182339809836, 16: 0.9974929474468809, 17: 0.964166000502184, 18: 0.9967109735913109, 19: 0.994714277726598, 20: 0.9981684394085314, 21: 0.9730360584463924, 22: 0.9925736276605167, 23: 0.9959616230572574, 24: 0.9932541564456188, 25: 0.9968068491402082, 26: 0.9943388196822344, 27: 0.9886328814402222, 28: 0.9924112147563137, 29: 0.9985804146205192, 30: 0.9953321123765781, 31: 0.9950303460296709, 32: 0.989819327079691, 33: 0.9936385888806545, 34: 0.9903942774292082, 35: 0.9985304254308576, 36: 0.9892522467491217, 37: 0.9851930956458673, 38: 0.9958755964385345, 39: 0.9919543799958191, 40: 0.9961062597543933, 41: 0.9663613795004785, 42: 0.9974402118930593, 43: 0.9983577645617188, 44: 0.9978164695856394, 45: 0.9974214771366678, 46: 0.999631763449841, 47: 0.9991779998012352, 48: 0.9662514227610082, 49: 0.8226183490604162, 50: 0.9979990308283595, 51: 0.9987296069676522, 52: 0.9779559557251633, 53: 0.9679382215682417, 54: 0.9568482094202191, 55: 0.9979824855398619, 56: 0.9921365033332259, 57: 0.9896425230735914, 58: 0.8200863848328592, 59: 0.9991935873477196, 60: 0.9930752964832573, 61: 0.9880242530142423, 62: 0.9985986346624559, 63: 0.9875996270407923, 64: 0.9981785856276983, 65: 0.9913152366080321, 66: 0.9622480838447809, 67: 0.9902284991086927, 68: 0.24378074699640323, 69: 0.9906790664317087, 70: 0.9984994175494648, 71: 0.9981016593386303, 72: 0.9994641577621515, 73: 0.9991363590023539, 74: 0.9959833249018993, 75: 0.9360660596154631, 76: 0.8579260414019227, 77: 0.9968846283899621, 78: 0.9997226174474927, 79: 0.9609592895368115, 80: 0.9965723512308905, 81: 0.9690753808729351, 82: 0.9779785528667272, 83: 0.999190073304635, 84: 0.9988078584680333, 85: 0.9896001534610986, 86: 0.9665683947112411, 87: 0.9960959486940411, 88: 0.9899340402865783, 89: 0.899452272117138, 90: 0.9957018788539572, 91: 0.9845092035774142, 92: 0.9996044865845324, 93: 0.9983523765014252, 94: 0.9886496723247692, 95: 0.9974172122376039, 96: 0.9932624642024748, 97: 0.9967399476942373, 98: 0.9991734256963828, 99: 0.9985334730903851}\n"
     ]
    }
   ],
   "source": [
    "CLASS_THRESHOLD = 0.95\n",
    "\n",
    "class_quantiles = {\n",
    "    i: CLASS_THRESHOLD\n",
    "    * ((len(class_conformity_scores[i]) + 1) / len(class_conformity_scores[i]))\n",
    "    for i in range(num_of_classes)\n",
    "}\n",
    "\n",
    "class_thresholds = {\n",
    "    i: np.quantile(class_conformity_scores[i], class_quantiles[i])\n",
    "    for i in range(num_of_classes)\n",
    "}\n",
    "\n",
    "print(class_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage of apple: 96.00%\n",
      "Coverage of aquarium_fish: 90.00%\n",
      "Coverage of baby: 90.00%\n",
      "Coverage of bear: 92.00%\n",
      "Coverage of beaver: 94.00%\n",
      "Coverage of bed: 92.00%\n",
      "Coverage of bee: 88.00%\n",
      "Coverage of beetle: 100.00%\n",
      "Coverage of bicycle: 96.00%\n",
      "Coverage of bottle: 94.00%\n",
      "Coverage of bowl: 92.00%\n",
      "Coverage of boy: 98.00%\n",
      "Coverage of bridge: 96.00%\n",
      "Coverage of bus: 92.00%\n",
      "Coverage of butterfly: 94.00%\n",
      "Coverage of camel: 94.00%\n",
      "Coverage of can: 100.00%\n",
      "Coverage of castle: 88.00%\n",
      "Coverage of caterpillar: 94.00%\n",
      "Coverage of cattle: 86.00%\n",
      "Coverage of chair: 100.00%\n",
      "Coverage of chimpanzee: 92.00%\n",
      "Coverage of clock: 94.00%\n",
      "Coverage of cloud: 98.00%\n",
      "Coverage of cockroach: 98.00%\n",
      "Coverage of couch: 98.00%\n",
      "Coverage of crab: 98.00%\n",
      "Coverage of crocodile: 94.00%\n",
      "Coverage of cup: 96.00%\n",
      "Coverage of dinosaur: 98.00%\n",
      "Coverage of dolphin: 90.00%\n",
      "Coverage of elephant: 98.00%\n",
      "Coverage of flatfish: 86.00%\n",
      "Coverage of forest: 90.00%\n",
      "Coverage of fox: 94.00%\n",
      "Coverage of girl: 92.00%\n",
      "Coverage of hamster: 96.00%\n",
      "Coverage of house: 94.00%\n",
      "Coverage of kangaroo: 98.00%\n",
      "Coverage of keyboard: 98.00%\n",
      "Coverage of lamp: 94.00%\n",
      "Coverage of lawn_mower: 94.00%\n",
      "Coverage of leopard: 100.00%\n",
      "Coverage of lion: 100.00%\n",
      "Coverage of lizard: 98.00%\n",
      "Coverage of lobster: 92.00%\n",
      "Coverage of man: 100.00%\n",
      "Coverage of maple_tree: 98.00%\n",
      "Coverage of motorcycle: 96.00%\n",
      "Coverage of mountain: 88.00%\n",
      "Coverage of mouse: 96.00%\n",
      "Coverage of mushroom: 100.00%\n",
      "Coverage of oak_tree: 88.00%\n",
      "Coverage of orange: 98.00%\n",
      "Coverage of orchid: 96.00%\n",
      "Coverage of otter: 92.00%\n",
      "Coverage of palm_tree: 100.00%\n",
      "Coverage of pear: 96.00%\n",
      "Coverage of pickup_truck: 90.00%\n",
      "Coverage of pine_tree: 96.00%\n",
      "Coverage of plain: 96.00%\n",
      "Coverage of plate: 94.00%\n",
      "Coverage of poppy: 98.00%\n",
      "Coverage of porcupine: 92.00%\n",
      "Coverage of possum: 94.00%\n",
      "Coverage of rabbit: 88.00%\n",
      "Coverage of raccoon: 84.00%\n",
      "Coverage of ray: 90.00%\n",
      "Coverage of road: 92.00%\n",
      "Coverage of rocket: 96.00%\n",
      "Coverage of rose: 94.00%\n",
      "Coverage of sea: 98.00%\n",
      "Coverage of seal: 100.00%\n",
      "Coverage of shark: 98.00%\n",
      "Coverage of shrew: 90.00%\n",
      "Coverage of skunk: 90.00%\n",
      "Coverage of skyscraper: 98.00%\n",
      "Coverage of snail: 98.00%\n",
      "Coverage of snake: 100.00%\n",
      "Coverage of spider: 96.00%\n",
      "Coverage of squirrel: 90.00%\n",
      "Coverage of streetcar: 90.00%\n",
      "Coverage of sunflower: 98.00%\n",
      "Coverage of sweet_pepper: 100.00%\n",
      "Coverage of table: 98.00%\n",
      "Coverage of tank: 96.00%\n",
      "Coverage of telephone: 84.00%\n",
      "Coverage of television: 98.00%\n",
      "Coverage of tiger: 94.00%\n",
      "Coverage of tractor: 96.00%\n",
      "Coverage of train: 100.00%\n",
      "Coverage of trout: 96.00%\n",
      "Coverage of tulip: 96.00%\n",
      "Coverage of turtle: 92.00%\n",
      "Coverage of wardrobe: 100.00%\n",
      "Coverage of whale: 94.00%\n",
      "Coverage of willow_tree: 96.00%\n",
      "Coverage of wolf: 98.00%\n",
      "Coverage of woman: 100.00%\n",
      "Coverage of worm: 100.00%\n",
      "Average coverage: 94.82%\n",
      "Average set size: 6.67\n"
     ]
    }
   ],
   "source": [
    "calculate_class_coverage(\n",
    "    resnet18, test_dl, class_thresholds, num_of_classes, classes_dict\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adaptive-vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
